<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-29T14:40:45+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Minish Lab</title><subtitle>Small models everywhere</subtitle><author><name>The Minish Lab</name></author><entry><title type="html">Tokenlearn: bag of tricks leads to better models</title><link href="http://localhost:4000/tokenlearn_blogpost/" rel="alternate" type="text/html" title="Tokenlearn: bag of tricks leads to better models" /><published>2024-10-29T00:00:00+01:00</published><updated>2024-10-29T00:00:00+01:00</updated><id>http://localhost:4000/tokenlearn_blogpost</id><content type="html" xml:base="http://localhost:4000/tokenlearn_blogpost/"><![CDATA[<p>This blogpost describes the <a href="https://github.com/MinishLab/tokenlearn">Tokenlearn</a> method, which is a method to pre-train Model2Vec models.</p>

<p>Weâ€™ve been brewing, concocting, distilling, and came up with a new distillation technique that leads to much better models, which we are now releasing under the name POTION. We open source all models, code, and data.</p>

<p>Weâ€™re releasing three versions: a 64-dim (1.9M params), 128-dim (3.8M params), and 256-dim (7.6M params) model, all based on the same base model, which is, in turn, a bge-base distillation. All POTION models outperform all previous distillations in their size class, and should be considered to be drop-in replacements of our M2V_base_output model. potion-base-8M, in particular, even improves over our largest model, M2V_base_glove. potion-base-8M is better than any set of static embeddings we could find on any task, including glove, fasttext and specialized word embeddings.</p>

<p>Get them here:</p>
<ul>
  <li><a href="https://huggingface.co/MinishLab/potion-base-8M">potion-base-8M</a></li>
  <li><a href="https://huggingface.co/MinishLab/potion-base-4M">potion-base-4M</a></li>
  <li><a href="https://huggingface.co/MinishLab/potion-base-2M">potion-base-2M</a></li>
</ul>

<p>The Tokenlearn code can be found <a href="https://github.com/MinishLab/tokenlearn">here</a>.</p>

<p>The rest of the post will detail how we made the models, how they perform, and further improvements we have in store.</p>

<h2 id="distillation">Distillation</h2>

<p>In our regular <a href="https://github.com/MinishLab/model2vec">model2vec</a> framework we distill sentence transformers down to really fast tiny models by doing a forward pass for all tokens separately. We then perform Principal Component Analysis (PCA) on the resulting embeddings, and weigh the individual embeddings via Zipfâ€™s law. See our previous blog post <a href="https://huggingface.co/blog/Pringled/model2vec">here</a>. The new distillation framework is composed of 4 steps.</p>

<ol>
  <li>Model2Vec distillation</li>
  <li>Sentence transformer inference</li>
  <li>Training</li>
  <li>Post-training regularization</li>
</ol>

<p>These four steps take a bit longer than the previous distillation framework. If you are looking for a quick way to get a model2vec model, distillation is still your best bet. If you are looking for maximum performance, read on!</p>

<h3 id="1-distillation">1. Distillation</h3>

<p>We start from a distilled model. In our case, we are using the M2V_base_output model as our starting point.</p>

<h3 id="2-sentence-transformer-inference">2. Sentence transformer inference</h3>

<p>We then go back to the original big sentence transformer, and use that transformer to create ~1M embeddings on an in-domain corpus, which for us is <a href="https://huggingface.co/datasets/allenai/c4">C4</a>. We then throw away the sentence transformer, never to see it again. Forget it existed.</p>

<h3 id="3-training">3. Training</h3>

<p>So, we now have a base model, and 1M texts and 1M vector representations of those texts. We then train the base model to minimize the cosine distance between the representations it produces and the representations we produced before. In doing so, our model learns to better mimic representations made by a large model. We also add a super heavy regularization term to the produced embeddings.</p>

<p>During training, we apply a few standard methods to improve performance, such as reducing the learning rate on plateau, and early stopping.</p>

<h3 id="4-post-training-re-regularization">4. Post-training re-regularization</h3>

<p>Finally, after training, we <em>re-regularize</em> our models by performing PCA, and by manually re-weighting individual tokens. As we show below, this massively improves performance, again.</p>

<p>Of note here is the manual re-weighting, which is very similar to the Zipf weighting we use, but now relies on external data. Before, we assumed that all tokens were in rank order, and simply weighted them as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">rank</span><span class="p">)</span>
</code></pre></div></div>

<p>This works really well, as shown in <a href="https://huggingface.co/blog/Pringled/model2vec">our original blog post</a>. Using actual frequencies, however, works even better. We use the same 1M documents on which we trained, and collect token probabilities for all tokens in our vocabulary. We then reweight using the following formula from the <a href="https://openreview.net/pdf?id=SyK00v5xx">SIF paper</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="mf">1e-3</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1e-3</span> <span class="o">+</span> <span class="n">proba</span><span class="p">)</span>
</code></pre></div></div>

<p>where <code class="language-plaintext highlighter-rouge">proba</code> is the probability of the token in the corpus. While this does mean our new distillation method relies on some data, it is <em>worth it</em>, as we will show below.</p>

<h2 id="results">Results</h2>

<p>Just like in our original experiments, we again evaluate on MTEB, as well as our two additional tasks (PEARL and WordSim). The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: right">Avg (All)</th>
      <th style="text-align: right">Avg (MTEB)</th>
      <th style="text-align: right">Class</th>
      <th style="text-align: right">Clust</th>
      <th style="text-align: right">PairClass</th>
      <th style="text-align: right">Rank</th>
      <th style="text-align: right">Ret</th>
      <th style="text-align: right">STS</th>
      <th style="text-align: right">Sum</th>
      <th style="text-align: right">Pearl</th>
      <th style="text-align: right">WordSim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">all-MiniLM-L6-v2</td>
      <td style="text-align: right">56.08</td>
      <td style="text-align: right">56.09</td>
      <td style="text-align: right">62.62</td>
      <td style="text-align: right">41.94</td>
      <td style="text-align: right">82.37</td>
      <td style="text-align: right">58.04</td>
      <td style="text-align: right">41.95</td>
      <td style="text-align: right">78.90</td>
      <td style="text-align: right">30.81</td>
      <td style="text-align: right">60.83</td>
      <td style="text-align: right">49.91</td>
    </tr>
    <tr>
      <td style="text-align: left">potion-base-8M</td>
      <td style="text-align: right">50.54</td>
      <td style="text-align: right">50.03</td>
      <td style="text-align: right">64.44</td>
      <td style="text-align: right">32.93</td>
      <td style="text-align: right">76.62</td>
      <td style="text-align: right">49.73</td>
      <td style="text-align: right">31.71</td>
      <td style="text-align: right">73.24</td>
      <td style="text-align: right">29.28</td>
      <td style="text-align: right">53.54</td>
      <td style="text-align: right">50.75</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove_subword</td>
      <td style="text-align: right">49.06</td>
      <td style="text-align: right">46.69</td>
      <td style="text-align: right">61.27</td>
      <td style="text-align: right">30.03</td>
      <td style="text-align: right">74.71</td>
      <td style="text-align: right">49.15</td>
      <td style="text-align: right">27.16</td>
      <td style="text-align: right">69.09</td>
      <td style="text-align: right">30.08</td>
      <td style="text-align: right">56.82</td>
      <td style="text-align: right">57.99</td>
    </tr>
    <tr>
      <td style="text-align: left">potion-base-4M</td>
      <td style="text-align: right">48.87</td>
      <td style="text-align: right">48.23</td>
      <td style="text-align: right">62.19</td>
      <td style="text-align: right">31.47</td>
      <td style="text-align: right">75.37</td>
      <td style="text-align: right">48.75</td>
      <td style="text-align: right">29.11</td>
      <td style="text-align: right">72.19</td>
      <td style="text-align: right">28.89</td>
      <td style="text-align: right">52.55</td>
      <td style="text-align: right">49.21</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove</td>
      <td style="text-align: right">48.58</td>
      <td style="text-align: right">47.6</td>
      <td style="text-align: right">61.35</td>
      <td style="text-align: right">30.52</td>
      <td style="text-align: right">75.34</td>
      <td style="text-align: right">48.5</td>
      <td style="text-align: right">29.26</td>
      <td style="text-align: right">70.31</td>
      <td style="text-align: right">31.5</td>
      <td style="text-align: right">50.28</td>
      <td style="text-align: right">54.29</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: right">46.79</td>
      <td style="text-align: right">45.34</td>
      <td style="text-align: right">61.25</td>
      <td style="text-align: right">25.58</td>
      <td style="text-align: right">74.9</td>
      <td style="text-align: right">47.63</td>
      <td style="text-align: right">26.14</td>
      <td style="text-align: right">68.58</td>
      <td style="text-align: right">29.2</td>
      <td style="text-align: right">54.02</td>
      <td style="text-align: right">49.18</td>
    </tr>
    <tr>
      <td style="text-align: left">potion-base-2M</td>
      <td style="text-align: right">45.52</td>
      <td style="text-align: right">44.77</td>
      <td style="text-align: right">58.45</td>
      <td style="text-align: right">27.5</td>
      <td style="text-align: right">73.72</td>
      <td style="text-align: right">46.82</td>
      <td style="text-align: right">24.13</td>
      <td style="text-align: right">70.14</td>
      <td style="text-align: right">31.51</td>
      <td style="text-align: right">50.82</td>
      <td style="text-align: right">44.72</td>
    </tr>
    <tr>
      <td style="text-align: left">GloVe_300d</td>
      <td style="text-align: right">42.84</td>
      <td style="text-align: right">42.36</td>
      <td style="text-align: right">57.31</td>
      <td style="text-align: right">27.66</td>
      <td style="text-align: right">72.48</td>
      <td style="text-align: right">43.3</td>
      <td style="text-align: right">22.78</td>
      <td style="text-align: right">61.9</td>
      <td style="text-align: right">28.81</td>
      <td style="text-align: right">45.65</td>
      <td style="text-align: right">43.05</td>
    </tr>
    <tr>
      <td style="text-align: left">BPEmb_50k_300d</td>
      <td style="text-align: right">39.34</td>
      <td style="text-align: right">37.78</td>
      <td style="text-align: right">55.76</td>
      <td style="text-align: right">23.35</td>
      <td style="text-align: right">57.86</td>
      <td style="text-align: right">43.21</td>
      <td style="text-align: right">17.5</td>
      <td style="text-align: right">55.1</td>
      <td style="text-align: right">29.74</td>
      <td style="text-align: right">47.56</td>
      <td style="text-align: right">41.28</td>
    </tr>
  </tbody>
</table>

<p>As can be seen, potion-base-8M is the best model we have released so far (surpassing the 50% average MTEB score mark!), further pushing the limits of what is possible with static word embeddings. Furthermore, the 4M and 2M models still work quite well, with the 2M model outperforming GloVE while being ~55 times smaller.</p>

<p>To show the relationship between the number of sentences per second and the average MTEB score, we plot the average MTEB score against sentences per second. The circle sizes correspond to the number of parameters in the models (larger = more parameters).</p>

<p><img src="/images/post_tokenlearn/speed_vs_mteb_score.png" alt="SpeedvsAccuracy" /> 
<em>The average MTEB score plotted against sentences per second. The circle size indicates model size.</em></p>]]></content><author><name>The Minish Lab</name></author><category term="Model2Vec" /><summary type="html"><![CDATA[This blogpost describes the Tokenlearn method, which is a method to pre-train Model2Vec models.]]></summary></entry><entry><title type="html">Model2Vec Introduction blogpost</title><link href="http://localhost:4000/hf_blogpost/" rel="alternate" type="text/html" title="Model2Vec Introduction blogpost" /><published>2024-10-14T00:00:00+02:00</published><updated>2024-10-14T00:00:00+02:00</updated><id>http://localhost:4000/hf_blogpost</id><content type="html" xml:base="http://localhost:4000/hf_blogpost/"><![CDATA[<p>This blog was first posted on the <a href="https://huggingface.co/blog/Pringled/model2vec">Hugging Face blog</a>. Weâ€™re also posting it here for archival purposes.</p>

<h1 id="model2vec-distill-a-small-fast-model-from-any-sentence-transformer">Model2Vec: Distill a Small Fast Model from any Sentence Transformer</h1>

<p>(Large) language models have become the de facto standard for feature extraction. While these models have shown state-of-the-art performance on a <a href="https://huggingface.co/spaces/mteb/leaderboard">large number of tasks</a> they also come with heavy resource requirements: large energy consumption, computational demands, and longer processing times. Although there are many ways in which you can make existing (Sentence) Transformers faster, e.g. quantization, or specialized kernels, they are still relatively slow, especially on CPU. What if you need to go faster and are working on a time-constrained product (e.g. a search engine), or have very little resources available?</p>

<p>This is where <a href="https://github.com/MinishLab/model2vec">Model2Vec</a> comes in â€” offering static embeddings that are hardware and eco-friendly while maintaining strong performance.</p>

<p>In this blog, we will discuss what Model2Vec is, how it works, how you can use it, and its performance.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/post_hf/ezlo_diagram_side.svg" alt="Model2Vec" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Visualization of the Model2Vec architecture.</em></td>
    </tr>
  </tbody>
</table>

<h3 id="table-of-contents">Table of Contents</h3>
<ul>
  <li><a href="#what-is-model2vec">What is model2vec?</a></li>
  <li><a href="#how-to-use-model2vec">How to use model2vec</a></li>
  <li><a href="#results">Results</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<h3 id="what-is-model2vec">What is Model2Vec?</h3>

<p>Model2Vec is a technique to distill a small, fast, high performance static model from any Sentence Transformer.  At a high level, it works by passing a vocabulary through a sentence transformer model, then reducing the dimensionality of the resulting embeddings using PCA, and finally weighting the embeddings using zipf weighting. No dataset is needed, just a model (and optionally, a vocabulary). During inference, we simply take the mean of all token embeddings occurring in a sentence. A Model2Vec model is therefore completely uncontextualized. While this may sound like a big downside, weâ€™ll show that it still performs quite well considering how small and fast it is.</p>

<p>The above might sound like a lot to you, so letâ€™s unpack this a little.</p>

<h4 id="transformers-and-embeddings">Transformers and embeddings</h4>

<p>In a sentence transformer encoding step, a string is first chopped up into subword tokens. The embeddings of these tokens are then fed through the model, which contextualizes them to create high-quality sentence representations. At the output, you get as many embeddings as you put in, so if your input sentence consists of 10 tokens, you also get 10 output tokens. These tokens are then turned into a sentence representation by a pooling mechanism, which can either be a simple mean, or a special pooler module.</p>

<p>On to Model2Vec: the project first started as a kind of cache for sentence transformers. Because a transformer vocabulary typically only has about 32k tokens, a word like <code class="language-plaintext highlighter-rouge">astoundingly</code> gets chopped up into four unique tokens: <code class="language-plaintext highlighter-rouge">'as', '##tou', '##nding', '##ly'</code>, which means that we re-compute the attention between those four tokens each time this word occurs. But the meaning of this word might not be ambiguous at all!</p>

<p>However, as we started implementing this, we noticed that you actually do not need to cache any words at all, and you can just use the output representations of individual tokens to get good sentence representations. And this is exactly what the basic mode of operation of Model2Vec is: for each of the 32k input tokens in a sentence transformer vocabulary, we do a forward pass, and then store the resulting embedding. For a new sentence, we then just take the mean of the token embeddings we computed.</p>

<p>Note that the output token representations of a model2vec model are uncontextualized. Unlike with normal transformer models, there is no way for the model to give different meanings to the same token in different contexts. While this might seem like a huge downside, we think that the actual context provides models with enough disambiguation potential.</p>

<p>In addition to this trick, we show that two additional tricks are necessary to get optimal performance.</p>

<h5 id="pca">PCA</h5>

<p>We reduce the dimensionality of the resulting token space by using Principal Component Analysis (PCA). Normally, using PCA is associated with a loss in performance, because you throw away information. However, in our case, reducing the dimensionality actually increased performance significantly. We think this is because PCA also normalizes the resulting space, in the sense of removing biases in the original vector space, thereby making it easier to learn from the vectors.</p>

<h5 id="zipf">Zipf</h5>

<p>As we take a simple mean over tokens in the space, it is important that the vectors are weighted correctly. Normally, a sentence transformer would be there to correctly weight all the tokens for us given the context, but we donâ€™t have that luxury any more. Intuitively, we would like to use something like Inverse Document Frequency (IDF) to down-weight very frequent or uninteresting words. But we donâ€™t have access to a corpus over which to compute document frequencies.</p>

<p>To overcome this, we opt to use a well-known principle from language sciences, which is that, given a frequency ranked list, the frequency of the items in that list follow a power law distribution. This is called Zipfâ€™s law. So, if we take the assumption that a vocabulary is ranked by frequency, we can accurately down-weight really frequent items without needing to have access to actual frequencies. As tokenizer vocabularies are sorted by frequency, we already have access to a ranked list, so this optimization can be applied without any additional work.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/post_hf/pca_zipf.svg" alt="PCAZipf" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Visualization of the effects of applying PCA and Zipf weighting on the embeddings.</em></td>
    </tr>
  </tbody>
</table>

<h3 id="usage">Usage</h3>

<p>The Model2Vec library has two broad modes of usage: <strong>distillation</strong> and <strong>inference</strong>. In distillation mode, you can distill your own model using any Sentence Transformer (and optionally your own vocabulary). In inference mode, you can use the distilled model (or use one of our pre-distilled models) to generate embeddings for your text data at extremely high speed.</p>

<p>There are three ways to distill a model:</p>
<ul>
  <li><strong>Output</strong>: behaves much like a real sentence transformer, i.e., it uses a subword tokenizer and simply encodes all wordpieces in its vocabulary. This is really quick to create (30 seconds on a CPU), very small (30 MB in float32), but might be less performant on some tasks.</li>
  <li><strong>Vocab (word)</strong>: In this mode, you can pass your own vocabulary to create representations. This allows you to create good representations for whatever in-domain data you have, and is a drop-in replacement for GloVe or word2vec.</li>
  <li><strong>Vocab (subword)</strong>: In this mode, you can pass your own vocabulary, but it also uses the subword vocabulary to create representations. This allows you to create good representations for whatever in-domain data you have.</li>
</ul>

<p>Note that, while vocabulary-based models are larger in terms of RAM, all models are equally fast, because our model is independent of vocabulary size.</p>

<p>Model2Vec embeddings can be used in a wide variety of applications, such as text classification, clustering, building a search engine, or a RAG system. They are an especially good fit for applications that require fast, lightweight embeddings with low resource requirements.</p>

<p>As we will show next, Model2Vec is very easy to use. It can either be used as a standalone package, or used directly in <a href="https://github.com/UKPLab/sentence-transformers">Sentence Transformers</a>. This means you can easily integrate it into any pipeline that supports Sentence Transformers (e.g. LangChain and LlamaIndex). You can also train model2vec models directly using Sentence Transformers, keeping the fast inference speed, but optimizing them directly for your use case.</p>

<h3 id="how-to-use-model2vec">How to use Model2Vec</h3>

<h4 id="installation">Installation</h4>

<p>Model2Vec can be installed using pip:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>model2vec
</code></pre></div></div>

<h4 id="usage-1">Usage</h4>

<h5 id="inference">Inference</h5>

<p>The easiest way to get started with Model2Vec is to download one of our flagship models from our <a href="https://huggingface.co/minishlab">HuggingFace hub</a>. These models are pre-trained and ready to use. The following code snippet shows how to load a model and make embeddings:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">model2vec</span> <span class="kn">import</span> <span class="n">StaticModel</span>

<span class="c1"># Load a model from the HuggingFace hub (in this case the M2V_base_output model)
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"minishlab/M2V_base_output"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">StaticModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Make embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"It's dangerous to go alone!"</span><span class="p">,</span> <span class="s">"It's a secret to everybody."</span><span class="p">])</span>

</code></pre></div></div>

<p>Or distill your own models and directly use them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">model2vec</span> <span class="kn">import</span> <span class="n">distill</span>

<span class="c1"># Choose a Sentence Transformer model
</span><span class="n">base_model_name</span> <span class="o">=</span> <span class="s">"BAAI/bge-base-en-v1.5"</span>

<span class="c1"># Distill an output model with the chosen dimensions
</span><span class="n">model</span> <span class="o">=</span> <span class="n">distill</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">pca_dims</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>

<span class="c1"># Make embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"supervillain Ganondorf has invaded Hyrule!"</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">"supervillain Ganondorf has invaded Hyrule!"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># ['super', '##vill', '##ain', 'gan', '##ond', '##orf', 'has', 'invaded', 'h', '##yr', '##ule', '!']
</span>
<span class="c1"># It looks like we split Ganondorf and Hyrule up into many subtokens
# To solve this, we can add these words to our vocabulary.
</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">[</span><span class="s">"supervillain"</span><span class="p">,</span> <span class="s">"ganondorf"</span><span class="p">,</span> <span class="s">"hyrule"</span><span class="p">]</span>

<span class="c1"># Distill the model with the custom vocabulary.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">distill</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">pca_dims</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">"supervillain Ganondorf has invaded Hyrule!"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># ['supervillain', 'ganondorf', 'has', 'invaded', 'hyrule', '!']
# Much better.
</span>
</code></pre></div></div>

<p>Model2Vec is also directly supported in <a href="https://github.com/UKPLab/sentence-transformers">Sentence Transformers</a>. To use Model2Vec in Sentence Transformers, you can initialize a <code class="language-plaintext highlighter-rouge">StaticEmbedding</code> class using <code class="language-plaintext highlighter-rouge">from_model2vec</code>. To directly distill in Sentence Transformers, the <code class="language-plaintext highlighter-rouge">StaticEmbedding</code> class can be initialized using <code class="language-plaintext highlighter-rouge">from_distillation</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.models</span> <span class="kn">import</span> <span class="n">StaticEmbedding</span>

<span class="c1"># Initialize a StaticEmbedding module using a pre-trained model
</span><span class="n">static_embedding</span> <span class="o">=</span> <span class="n">StaticEmbedding</span><span class="p">.</span><span class="n">from_model2vec</span><span class="p">(</span><span class="s">"minishlab/M2V_base_output"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="p">[</span><span class="n">static_embedding</span><span class="p">])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"It's dangerous to go alone!"</span><span class="p">,</span> <span class="s">"It's a secret to everybody."</span><span class="p">])</span>

<span class="c1"># Or distill your own directly without leaving sentence-transformers
</span><span class="n">static_embedding</span> <span class="o">=</span> <span class="n">StaticEmbedding</span><span class="p">.</span><span class="n">from_distillation</span><span class="p">(</span><span class="s">"BAAI/bge-base-en-v1.5"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cpu"</span><span class="p">,</span> <span class="n">pca_dims</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="p">[</span><span class="n">static_embedding</span><span class="p">])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"It's dangerous to go alone!"</span><span class="p">,</span> <span class="s">"It's a secret to everybody."</span><span class="p">])</span>

</code></pre></div></div>

<h3 id="results">Results</h3>

<p>We evaluated Model2Vec on a large number of tasks and datasets. Model2Vec is evaluated on MTEB, as well as two additional tasks: <a href="https://arxiv.org/pdf/2401.10407">PEARL</a> (a phrase representation task) and WordSim (a collection of word similarity tasks). The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">Avg (All)</th>
      <th style="text-align: center">Avg (MTEB)</th>
      <th style="text-align: center">Class</th>
      <th style="text-align: center">Clust</th>
      <th style="text-align: center">PairClass</th>
      <th style="text-align: center">Rank</th>
      <th style="text-align: center">Ret</th>
      <th style="text-align: center">STS</th>
      <th style="text-align: center">Sum</th>
      <th style="text-align: center">Pearl</th>
      <th style="text-align: center">WordSim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">all-MiniLM-L6-v2</td>
      <td style="text-align: center">56.08</td>
      <td style="text-align: center">56.09</td>
      <td style="text-align: center">62.62</td>
      <td style="text-align: center">41.94</td>
      <td style="text-align: center">82.37</td>
      <td style="text-align: center">58.04</td>
      <td style="text-align: center">41.95</td>
      <td style="text-align: center">78.90</td>
      <td style="text-align: center">30.81</td>
      <td style="text-align: center">60.83</td>
      <td style="text-align: center">49.91</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove_subword</td>
      <td style="text-align: center">49.06</td>
      <td style="text-align: center">46.69</td>
      <td style="text-align: center">61.27</td>
      <td style="text-align: center">30.03</td>
      <td style="text-align: center">74.71</td>
      <td style="text-align: center">49.15</td>
      <td style="text-align: center">27.16</td>
      <td style="text-align: center">69.09</td>
      <td style="text-align: center">30.08</td>
      <td style="text-align: center">56.82</td>
      <td style="text-align: center">57.99</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove</td>
      <td style="text-align: center">48.58</td>
      <td style="text-align: center">47.60</td>
      <td style="text-align: center">61.35</td>
      <td style="text-align: center">30.52</td>
      <td style="text-align: center">75.34</td>
      <td style="text-align: center">48.50</td>
      <td style="text-align: center">29.26</td>
      <td style="text-align: center">70.31</td>
      <td style="text-align: center">31.50</td>
      <td style="text-align: center">50.28</td>
      <td style="text-align: center">54.29</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: center">46.79</td>
      <td style="text-align: center">45.34</td>
      <td style="text-align: center">61.25</td>
      <td style="text-align: center">25.58</td>
      <td style="text-align: center">74.90</td>
      <td style="text-align: center">47.63</td>
      <td style="text-align: center">26.14</td>
      <td style="text-align: center">68.58</td>
      <td style="text-align: center">29.20</td>
      <td style="text-align: center">54.02</td>
      <td style="text-align: center">49.18</td>
    </tr>
    <tr>
      <td style="text-align: left">GloVe_300d</td>
      <td style="text-align: center">42.84</td>
      <td style="text-align: center">42.36</td>
      <td style="text-align: center">57.31</td>
      <td style="text-align: center">27.66</td>
      <td style="text-align: center">72.48</td>
      <td style="text-align: center">43.30</td>
      <td style="text-align: center">22.78</td>
      <td style="text-align: center">61.90</td>
      <td style="text-align: center">28.81</td>
      <td style="text-align: center">45.65</td>
      <td style="text-align: center">43.05</td>
    </tr>
    <tr>
      <td style="text-align: left">BPEmb_50k_300d</td>
      <td style="text-align: center">39.34</td>
      <td style="text-align: center">37.78</td>
      <td style="text-align: center">55.76</td>
      <td style="text-align: center">23.35</td>
      <td style="text-align: center">57.86</td>
      <td style="text-align: center">43.21</td>
      <td style="text-align: center">17.50</td>
      <td style="text-align: center">55.10</td>
      <td style="text-align: center">29.74</td>
      <td style="text-align: center">47.56</td>
      <td style="text-align: center">41.28</td>
    </tr>
  </tbody>
</table>

<p>As can be seen, Model2Vec significantly outperforms GloVe and BPEmb on all tasks, and even outperforms MiniLM, which is a much slower model, on some tasks.</p>

<p>In addition, we evaluated Model2Vec on a number of classification datasets that are not in MTEB. We also use these to benchmark the speed of the model. The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">Average</th>
      <th style="text-align: center">SST2</th>
      <th style="text-align: center">IMDB</th>
      <th style="text-align: center">TREC</th>
      <th style="text-align: center">AG News</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">bge-base-en-v1.5</td>
      <td style="text-align: center">90.00</td>
      <td style="text-align: center">91.54</td>
      <td style="text-align: center">91.88</td>
      <td style="text-align: center">85.16</td>
      <td style="text-align: center">91.45</td>
    </tr>
    <tr>
      <td style="text-align: left">all-MiniLM-L6-v2</td>
      <td style="text-align: center">84.10</td>
      <td style="text-align: center">83.95</td>
      <td style="text-align: center">81.36</td>
      <td style="text-align: center">81.31</td>
      <td style="text-align: center">89.77</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: center">82.23</td>
      <td style="text-align: center">80.92</td>
      <td style="text-align: center">84.56</td>
      <td style="text-align: center">75.27</td>
      <td style="text-align: center">88.17</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove_subword</td>
      <td style="text-align: center">81.95</td>
      <td style="text-align: center">82.84</td>
      <td style="text-align: center">85.96</td>
      <td style="text-align: center">70.51</td>
      <td style="text-align: center">88.49</td>
    </tr>
    <tr>
      <td style="text-align: left">BPEmb_50k_300d</td>
      <td style="text-align: center">81.15</td>
      <td style="text-align: center">80.42</td>
      <td style="text-align: center">84.04</td>
      <td style="text-align: center">71.25</td>
      <td style="text-align: center">88.92</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove</td>
      <td style="text-align: center">80.76</td>
      <td style="text-align: center">83.07</td>
      <td style="text-align: center">85.24</td>
      <td style="text-align: center">66.12</td>
      <td style="text-align: center">88.61</td>
    </tr>
    <tr>
      <td style="text-align: left">GloVe_300d</td>
      <td style="text-align: center">77.77</td>
      <td style="text-align: center">81.68</td>
      <td style="text-align: center">84.00</td>
      <td style="text-align: center">55.67</td>
      <td style="text-align: center">89.71</td>
    </tr>
  </tbody>
</table>

<p>Again, Model2Vec outperforms GloVe BPEmb on all tasks, and even shows similar performance to MiniLM.</p>

<p>The figure below shows the relationship between the number of sentences per second and the average classification score. The circle sizes correspond to the number of parameters in the models (larger = more parameters). This plot shows that the Model2Vec models are much faster than the other models, while still being competitive in terms of classification performance with the all-MiniLM-L6-v2 model.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/post_hf/speed_vs_accuracy.png" alt="SpeedvsAccuracy" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>The average accuracy over all classification datasets plotted against sentence per second. The circle size indicates model size.</em></td>
    </tr>
  </tbody>
</table>

<h4 id="ablations">Ablations</h4>

<p>To better understand the factors contributing to the performance of Model2Vec, we conducted a comprehensive set of ablation studies, covering various aspects of the modelâ€™s architecture and preprocessing methods. In these studies, we examined the impact of key elements such as PCA, Zipf weighting, and the use of Sentence Transformers versus regular transformer models. We also compared the performance of input embeddings versus output embeddings, since it would seem plausible that these should also work well. The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: right">Avg (All)</th>
      <th style="text-align: right">Avg (MTEB)</th>
      <th style="text-align: right">Class</th>
      <th style="text-align: right">Clust</th>
      <th style="text-align: right">PairClass</th>
      <th style="text-align: right">Rank</th>
      <th style="text-align: right">Ret</th>
      <th style="text-align: right">STS</th>
      <th style="text-align: right">Sum</th>
      <th style="text-align: right">Pearl</th>
      <th style="text-align: right">WordSim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: right">46.79</td>
      <td style="text-align: right">45.34</td>
      <td style="text-align: right">61.25</td>
      <td style="text-align: right">25.58</td>
      <td style="text-align: right">74.9</td>
      <td style="text-align: right">47.63</td>
      <td style="text-align: right">26.14</td>
      <td style="text-align: right">68.58</td>
      <td style="text-align: right">29.2</td>
      <td style="text-align: right">54.02</td>
      <td style="text-align: right">49.18</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output_nopca</td>
      <td style="text-align: right">44.04</td>
      <td style="text-align: right">42.31</td>
      <td style="text-align: right">61.42</td>
      <td style="text-align: right">20.15</td>
      <td style="text-align: right">68.21</td>
      <td style="text-align: right">44.67</td>
      <td style="text-align: right">25.25</td>
      <td style="text-align: right">61.87</td>
      <td style="text-align: right">29.85</td>
      <td style="text-align: right">51.02</td>
      <td style="text-align: right">48.96</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output_nozipf</td>
      <td style="text-align: right">43.61</td>
      <td style="text-align: right">41.52</td>
      <td style="text-align: right">60.44</td>
      <td style="text-align: right">21.62</td>
      <td style="text-align: right">72.15</td>
      <td style="text-align: right">45.57</td>
      <td style="text-align: right">20.35</td>
      <td style="text-align: right">62.71</td>
      <td style="text-align: right">30.66</td>
      <td style="text-align: right">52.28</td>
      <td style="text-align: right">49.17</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_input_nozipf_nopca</td>
      <td style="text-align: right">40.97</td>
      <td style="text-align: right">39.55</td>
      <td style="text-align: right">54.16</td>
      <td style="text-align: right">18.62</td>
      <td style="text-align: right">68.3</td>
      <td style="text-align: right">43.65</td>
      <td style="text-align: right">23.63</td>
      <td style="text-align: right">59.38</td>
      <td style="text-align: right">32.04</td>
      <td style="text-align: right">50.19</td>
      <td style="text-align: right">40.52</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output_nozipf_nopca</td>
      <td style="text-align: right">40.8</td>
      <td style="text-align: right">38.44</td>
      <td style="text-align: right">59.78</td>
      <td style="text-align: right">19.31</td>
      <td style="text-align: right">62.39</td>
      <td style="text-align: right">42.26</td>
      <td style="text-align: right">19.01</td>
      <td style="text-align: right">55.16</td>
      <td style="text-align: right">30</td>
      <td style="text-align: right">49.09</td>
      <td style="text-align: right">48.97</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_input</td>
      <td style="text-align: right">40.74</td>
      <td style="text-align: right">39.93</td>
      <td style="text-align: right">60.35</td>
      <td style="text-align: right">22.66</td>
      <td style="text-align: right">59.63</td>
      <td style="text-align: right">43.02</td>
      <td style="text-align: right">25.47</td>
      <td style="text-align: right">50.05</td>
      <td style="text-align: right">29.35</td>
      <td style="text-align: right">50.61</td>
      <td style="text-align: right">34.47</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_bert_output_nozipf_nopca</td>
      <td style="text-align: right">35.54</td>
      <td style="text-align: right">34.82</td>
      <td style="text-align: right">55.69</td>
      <td style="text-align: right">15.42</td>
      <td style="text-align: right">58.68</td>
      <td style="text-align: right">39.87</td>
      <td style="text-align: right">12.92</td>
      <td style="text-align: right">55.24</td>
      <td style="text-align: right">30.15</td>
      <td style="text-align: right">46.9</td>
      <td style="text-align: right">26.72</td>
    </tr>
  </tbody>
</table>

<p>Thereâ€™s four main findings in these results:</p>
<ol>
  <li>Non-Sentence Transformers do not work well. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_bert_output_nozipf_nopca</code> (which uses <a href="https://huggingface.co/google-bert/bert-base-uncased">BERT</a>, a non-Sentence Transformer) and <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf_nopca</code> (which uses <a href="https://huggingface.co/BAAI/bge-base-en-v1.5">BGE-base</a>, a Sentence Transformer). Using a Sentence Transformer gives a ~5.2% increase in performance.</li>
  <li>PCA is crucial for performance. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf_nopca</code> and <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf</code> which gives a ~2.8% increase in performance. Furthermore, PCA improves performance on <em>all</em> tasks.</li>
  <li>Zipf weighting is crucial for performance. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf_nopca</code> and <code class="language-plaintext highlighter-rouge">M2V_base_output_nopca</code> which gives a ~3.1% increase in performance.</li>
  <li>Output embeddings outperform input embeddings. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_base_input</code> and <code class="language-plaintext highlighter-rouge">M2V_base_output</code> which gives a ~6.1% increase in performance. Note that input embeddings do work well for some tasks. We hypothesize that this is because input embeddings are inherently normalized.</li>
</ol>

<h3 id="conclusion">Conclusion</h3>

<p>Thanks for reading our blog post on Model2Vec! We hope you found it informative and useful. If you have any questions or comments, please feel free to reach out to us. We are still actively working on the project, and have a number of features already planned, so stay tuned.</p>

<ul>
  <li>ðŸ’» <a href="https://github.com/MinishLab/model2vec">Repository</a></li>
  <li>ðŸ¤— <a href="https://huggingface.co/minishlab">HuggingFace Org</a></li>
  <li>ðŸ¤— <a href="https://huggingface.co/collections/minishlab/model2vec-base-models-66fd9dd9b7c3b3c0f25ca90e">HuggingFace Models</a></li>
  <li>ðŸ‘¥ <a href="https://www.linkedin.com/company/minish-lab">LinkedIn</a></li>
  <li>ðŸ“š <a href="https://github.com/MinishLab/model2vec/tree/main/tutorials">Tutorials</a></li>
</ul>

<h3 id="citing">Citing</h3>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024word2vec</span><span class="p">,</span>
  <span class="na">authors</span> <span class="p">=</span> <span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://github.com/MinishLab/model2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>Weâ€™d like to thank <a href="https://huggingface.co/tomaarsen">Tom Aarsen</a> for integrating Model2Vec into <a href="https://github.com/UKPLab/sentence-transformers">Sentence Transformers</a> and helping us with our <a href="https://huggingface.co/minishlab">HuggingFace</a> integration, as well as his general feedback on the project.</p>]]></content><author><name>The Minish Lab</name></author><category term="Model2Vec" /><summary type="html"><![CDATA[This blog was first posted on the Hugging Face blog. Weâ€™re also posting it here for archival purposes.]]></summary></entry></feed>