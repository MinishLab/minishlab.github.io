<!DOCTYPE html>
<html>
  <head>
    <title>Model2Vec Introduction blogpost – Minish Lab – Small models everywhere</title>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Model2Vec Introduction blogpost" />
<meta name="author" content="The Minish Lab" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog was first posted on the Hugging Face blog. We’re also posting it here for archival purposes." />
<meta property="og:description" content="This blog was first posted on the Hugging Face blog. We’re also posting it here for archival purposes." />
<link rel="canonical" href="http://localhost:4000/hf_blogpost/" />
<meta property="og:url" content="http://localhost:4000/hf_blogpost/" />
<meta property="og:site_name" content="Minish Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-14T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Model2Vec Introduction blogpost" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"The Minish Lab"},"dateModified":"2024-10-14T00:00:00+02:00","datePublished":"2024-10-14T00:00:00+02:00","description":"This blog was first posted on the Hugging Face blog. We’re also posting it here for archival purposes.","headline":"Model2Vec Introduction blogpost","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/hf_blogpost/"},"url":"http://localhost:4000/hf_blogpost/"}</script>
<!-- End Jekyll SEO tag -->

    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>


<meta property="og:description" content="This blog was first posted on the Hugging Face blog. We’re also posting it here for archival purposes.

" />

<meta name="author" content="Minish Lab" />


<meta property="og:title" content="Model2Vec Introduction blogpost" />
<meta property="twitter:title" content="Model2Vec Introduction blogpost" />



<meta property="og:image" content="http://localhost:4000/images/ezlo.png"/>
<meta property="twitter:image" content="http://localhost:4000/images/ezlo.png"/>



    <link rel="stylesheet" type="text/css" href="/assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Minish Lab - Small models everywhere" href="/feed.xml" />
    <link rel="canonical" href="http://localhost:4000/hf_blogpost/" />

    <meta name="theme-color" content="#000000">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="/" class="site-avatar"><img src="/images/ezlo.png" alt="" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="/">Minish Lab</a></h1>
              <p class="site-description">Small models everywhere</p> 
            </div>

            <nav>
              <a href="/">Home</a>
              <a href="/about">About</a>
              <a href="/archive">Archive</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <article class="post detailed">
  <h1>Model2Vec Introduction blogpost</h1>

  <div>
    <p class="author_title">The Minish Lab  ·  October 14, 2024</p>
    
    <div class="post-tags">
      
    </div>
  </div>
    
  <div class="entry">
    <p>This blog was first posted on the <a href="https://huggingface.co/blog/Pringled/model2vec">Hugging Face blog</a>. We’re also posting it here for archival purposes.</p>

<h1 id="model2vec-distill-a-small-fast-model-from-any-sentence-transformer">Model2Vec: Distill a Small Fast Model from any Sentence Transformer</h1>

<p>(Large) language models have become the de facto standard for feature extraction. While these models have shown state-of-the-art performance on a <a href="https://huggingface.co/spaces/mteb/leaderboard">large number of tasks</a> they also come with heavy resource requirements: large energy consumption, computational demands, and longer processing times. Although there are many ways in which you can make existing (Sentence) Transformers faster, e.g. quantization, or specialized kernels, they are still relatively slow, especially on CPU. What if you need to go faster and are working on a time-constrained product (e.g. a search engine), or have very little resources available?</p>

<p>This is where <a href="https://github.com/MinishLab/model2vec">Model2Vec</a> comes in — offering static embeddings that are hardware and eco-friendly while maintaining strong performance.</p>

<p>In this blog, we will discuss what Model2Vec is, how it works, how you can use it, and its performance.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/post_hf/ezlo_diagram_side.svg" alt="Model2Vec" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Visualization of the Model2Vec architecture.</em></td>
    </tr>
  </tbody>
</table>

<h3 id="table-of-contents">Table of Contents</h3>
<ul>
  <li><a href="#what-is-model2vec">What is model2vec?</a></li>
  <li><a href="#how-to-use-model2vec">How to use model2vec</a></li>
  <li><a href="#results">Results</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<h3 id="what-is-model2vec">What is Model2Vec?</h3>

<p>Model2Vec is a technique to distill a small, fast, high performance static model from any Sentence Transformer.  At a high level, it works by passing a vocabulary through a sentence transformer model, then reducing the dimensionality of the resulting embeddings using PCA, and finally weighting the embeddings using zipf weighting. No dataset is needed, just a model (and optionally, a vocabulary). During inference, we simply take the mean of all token embeddings occurring in a sentence. A Model2Vec model is therefore completely uncontextualized. While this may sound like a big downside, we’ll show that it still performs quite well considering how small and fast it is.</p>

<p>The above might sound like a lot to you, so let’s unpack this a little.</p>

<h4 id="transformers-and-embeddings">Transformers and embeddings</h4>

<p>In a sentence transformer encoding step, a string is first chopped up into subword tokens. The embeddings of these tokens are then fed through the model, which contextualizes them to create high-quality sentence representations. At the output, you get as many embeddings as you put in, so if your input sentence consists of 10 tokens, you also get 10 output tokens. These tokens are then turned into a sentence representation by a pooling mechanism, which can either be a simple mean, or a special pooler module.</p>

<p>On to Model2Vec: the project first started as a kind of cache for sentence transformers. Because a transformer vocabulary typically only has about 32k tokens, a word like <code class="language-plaintext highlighter-rouge">astoundingly</code> gets chopped up into four unique tokens: <code class="language-plaintext highlighter-rouge">'as', '##tou', '##nding', '##ly'</code>, which means that we re-compute the attention between those four tokens each time this word occurs. But the meaning of this word might not be ambiguous at all!</p>

<p>However, as we started implementing this, we noticed that you actually do not need to cache any words at all, and you can just use the output representations of individual tokens to get good sentence representations. And this is exactly what the basic mode of operation of Model2Vec is: for each of the 32k input tokens in a sentence transformer vocabulary, we do a forward pass, and then store the resulting embedding. For a new sentence, we then just take the mean of the token embeddings we computed.</p>

<p>Note that the output token representations of a model2vec model are uncontextualized. Unlike with normal transformer models, there is no way for the model to give different meanings to the same token in different contexts. While this might seem like a huge downside, we think that the actual context provides models with enough disambiguation potential.</p>

<p>In addition to this trick, we show that two additional tricks are necessary to get optimal performance.</p>

<h5 id="pca">PCA</h5>

<p>We reduce the dimensionality of the resulting token space by using Principal Component Analysis (PCA). Normally, using PCA is associated with a loss in performance, because you throw away information. However, in our case, reducing the dimensionality actually increased performance significantly. We think this is because PCA also normalizes the resulting space, in the sense of removing biases in the original vector space, thereby making it easier to learn from the vectors.</p>

<h5 id="zipf">Zipf</h5>

<p>As we take a simple mean over tokens in the space, it is important that the vectors are weighted correctly. Normally, a sentence transformer would be there to correctly weight all the tokens for us given the context, but we don’t have that luxury any more. Intuitively, we would like to use something like Inverse Document Frequency (IDF) to down-weight very frequent or uninteresting words. But we don’t have access to a corpus over which to compute document frequencies.</p>

<p>To overcome this, we opt to use a well-known principle from language sciences, which is that, given a frequency ranked list, the frequency of the items in that list follow a power law distribution. This is called Zipf’s law. So, if we take the assumption that a vocabulary is ranked by frequency, we can accurately down-weight really frequent items without needing to have access to actual frequencies. As tokenizer vocabularies are sorted by frequency, we already have access to a ranked list, so this optimization can be applied without any additional work.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/post_hf/pca_zipf.svg" alt="PCAZipf" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>Visualization of the effects of applying PCA and Zipf weighting on the embeddings.</em></td>
    </tr>
  </tbody>
</table>

<h3 id="usage">Usage</h3>

<p>The Model2Vec library has two broad modes of usage: <strong>distillation</strong> and <strong>inference</strong>. In distillation mode, you can distill your own model using any Sentence Transformer (and optionally your own vocabulary). In inference mode, you can use the distilled model (or use one of our pre-distilled models) to generate embeddings for your text data at extremely high speed.</p>

<p>There are three ways to distill a model:</p>
<ul>
  <li><strong>Output</strong>: behaves much like a real sentence transformer, i.e., it uses a subword tokenizer and simply encodes all wordpieces in its vocabulary. This is really quick to create (30 seconds on a CPU), very small (30 MB in float32), but might be less performant on some tasks.</li>
  <li><strong>Vocab (word)</strong>: In this mode, you can pass your own vocabulary to create representations. This allows you to create good representations for whatever in-domain data you have, and is a drop-in replacement for GloVe or word2vec.</li>
  <li><strong>Vocab (subword)</strong>: In this mode, you can pass your own vocabulary, but it also uses the subword vocabulary to create representations. This allows you to create good representations for whatever in-domain data you have.</li>
</ul>

<p>Note that, while vocabulary-based models are larger in terms of RAM, all models are equally fast, because our model is independent of vocabulary size.</p>

<p>Model2Vec embeddings can be used in a wide variety of applications, such as text classification, clustering, building a search engine, or a RAG system. They are an especially good fit for applications that require fast, lightweight embeddings with low resource requirements.</p>

<p>As we will show next, Model2Vec is very easy to use. It can either be used as a standalone package, or used directly in <a href="https://github.com/UKPLab/sentence-transformers">Sentence Transformers</a>. This means you can easily integrate it into any pipeline that supports Sentence Transformers (e.g. LangChain and LlamaIndex). You can also train model2vec models directly using Sentence Transformers, keeping the fast inference speed, but optimizing them directly for your use case.</p>

<h3 id="how-to-use-model2vec">How to use Model2Vec</h3>

<h4 id="installation">Installation</h4>

<p>Model2Vec can be installed using pip:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>model2vec
</code></pre></div></div>

<h4 id="usage-1">Usage</h4>

<h5 id="inference">Inference</h5>

<p>The easiest way to get started with Model2Vec is to download one of our flagship models from our <a href="https://huggingface.co/minishlab">HuggingFace hub</a>. These models are pre-trained and ready to use. The following code snippet shows how to load a model and make embeddings:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">model2vec</span> <span class="kn">import</span> <span class="n">StaticModel</span>

<span class="c1"># Load a model from the HuggingFace hub (in this case the M2V_base_output model)
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">"minishlab/M2V_base_output"</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">StaticModel</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Make embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"It's dangerous to go alone!"</span><span class="p">,</span> <span class="s">"It's a secret to everybody."</span><span class="p">])</span>

</code></pre></div></div>

<p>Or distill your own models and directly use them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">model2vec</span> <span class="kn">import</span> <span class="n">distill</span>

<span class="c1"># Choose a Sentence Transformer model
</span><span class="n">base_model_name</span> <span class="o">=</span> <span class="s">"BAAI/bge-base-en-v1.5"</span>

<span class="c1"># Distill an output model with the chosen dimensions
</span><span class="n">model</span> <span class="o">=</span> <span class="n">distill</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">pca_dims</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>

<span class="c1"># Make embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"supervillain Ganondorf has invaded Hyrule!"</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">"supervillain Ganondorf has invaded Hyrule!"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># ['super', '##vill', '##ain', 'gan', '##ond', '##orf', 'has', 'invaded', 'h', '##yr', '##ule', '!']
</span>
<span class="c1"># It looks like we split Ganondorf and Hyrule up into many subtokens
# To solve this, we can add these words to our vocabulary.
</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">[</span><span class="s">"supervillain"</span><span class="p">,</span> <span class="s">"ganondorf"</span><span class="p">,</span> <span class="s">"hyrule"</span><span class="p">]</span>

<span class="c1"># Distill the model with the custom vocabulary.
</span><span class="n">model</span> <span class="o">=</span> <span class="n">distill</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">base_model_name</span><span class="p">,</span> <span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">pca_dims</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="s">"supervillain Ganondorf has invaded Hyrule!"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># ['supervillain', 'ganondorf', 'has', 'invaded', 'hyrule', '!']
# Much better.
</span>
</code></pre></div></div>

<p>Model2Vec is also directly supported in <a href="https://github.com/UKPLab/sentence-transformers">Sentence Transformers</a>. To use Model2Vec in Sentence Transformers, you can initialize a <code class="language-plaintext highlighter-rouge">StaticEmbedding</code> class using <code class="language-plaintext highlighter-rouge">from_model2vec</code>. To directly distill in Sentence Transformers, the <code class="language-plaintext highlighter-rouge">StaticEmbedding</code> class can be initialized using <code class="language-plaintext highlighter-rouge">from_distillation</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.models</span> <span class="kn">import</span> <span class="n">StaticEmbedding</span>

<span class="c1"># Initialize a StaticEmbedding module using a pre-trained model
</span><span class="n">static_embedding</span> <span class="o">=</span> <span class="n">StaticEmbedding</span><span class="p">.</span><span class="n">from_model2vec</span><span class="p">(</span><span class="s">"minishlab/M2V_base_output"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="p">[</span><span class="n">static_embedding</span><span class="p">])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"It's dangerous to go alone!"</span><span class="p">,</span> <span class="s">"It's a secret to everybody."</span><span class="p">])</span>

<span class="c1"># Or distill your own directly without leaving sentence-transformers
</span><span class="n">static_embedding</span> <span class="o">=</span> <span class="n">StaticEmbedding</span><span class="p">.</span><span class="n">from_distillation</span><span class="p">(</span><span class="s">"BAAI/bge-base-en-v1.5"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cpu"</span><span class="p">,</span> <span class="n">pca_dims</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="p">[</span><span class="n">static_embedding</span><span class="p">])</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">encode</span><span class="p">([</span><span class="s">"It's dangerous to go alone!"</span><span class="p">,</span> <span class="s">"It's a secret to everybody."</span><span class="p">])</span>

</code></pre></div></div>

<h3 id="results">Results</h3>

<p>We evaluated Model2Vec on a large number of tasks and datasets. Model2Vec is evaluated on MTEB, as well as two additional tasks: <a href="https://arxiv.org/pdf/2401.10407">PEARL</a> (a phrase representation task) and WordSim (a collection of word similarity tasks). The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">Avg (All)</th>
      <th style="text-align: center">Avg (MTEB)</th>
      <th style="text-align: center">Class</th>
      <th style="text-align: center">Clust</th>
      <th style="text-align: center">PairClass</th>
      <th style="text-align: center">Rank</th>
      <th style="text-align: center">Ret</th>
      <th style="text-align: center">STS</th>
      <th style="text-align: center">Sum</th>
      <th style="text-align: center">Pearl</th>
      <th style="text-align: center">WordSim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">all-MiniLM-L6-v2</td>
      <td style="text-align: center">56.08</td>
      <td style="text-align: center">56.09</td>
      <td style="text-align: center">62.62</td>
      <td style="text-align: center">41.94</td>
      <td style="text-align: center">82.37</td>
      <td style="text-align: center">58.04</td>
      <td style="text-align: center">41.95</td>
      <td style="text-align: center">78.90</td>
      <td style="text-align: center">30.81</td>
      <td style="text-align: center">60.83</td>
      <td style="text-align: center">49.91</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove_subword</td>
      <td style="text-align: center">49.06</td>
      <td style="text-align: center">46.69</td>
      <td style="text-align: center">61.27</td>
      <td style="text-align: center">30.03</td>
      <td style="text-align: center">74.71</td>
      <td style="text-align: center">49.15</td>
      <td style="text-align: center">27.16</td>
      <td style="text-align: center">69.09</td>
      <td style="text-align: center">30.08</td>
      <td style="text-align: center">56.82</td>
      <td style="text-align: center">57.99</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove</td>
      <td style="text-align: center">48.58</td>
      <td style="text-align: center">47.60</td>
      <td style="text-align: center">61.35</td>
      <td style="text-align: center">30.52</td>
      <td style="text-align: center">75.34</td>
      <td style="text-align: center">48.50</td>
      <td style="text-align: center">29.26</td>
      <td style="text-align: center">70.31</td>
      <td style="text-align: center">31.50</td>
      <td style="text-align: center">50.28</td>
      <td style="text-align: center">54.29</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: center">46.79</td>
      <td style="text-align: center">45.34</td>
      <td style="text-align: center">61.25</td>
      <td style="text-align: center">25.58</td>
      <td style="text-align: center">74.90</td>
      <td style="text-align: center">47.63</td>
      <td style="text-align: center">26.14</td>
      <td style="text-align: center">68.58</td>
      <td style="text-align: center">29.20</td>
      <td style="text-align: center">54.02</td>
      <td style="text-align: center">49.18</td>
    </tr>
    <tr>
      <td style="text-align: left">GloVe_300d</td>
      <td style="text-align: center">42.84</td>
      <td style="text-align: center">42.36</td>
      <td style="text-align: center">57.31</td>
      <td style="text-align: center">27.66</td>
      <td style="text-align: center">72.48</td>
      <td style="text-align: center">43.30</td>
      <td style="text-align: center">22.78</td>
      <td style="text-align: center">61.90</td>
      <td style="text-align: center">28.81</td>
      <td style="text-align: center">45.65</td>
      <td style="text-align: center">43.05</td>
    </tr>
    <tr>
      <td style="text-align: left">BPEmb_50k_300d</td>
      <td style="text-align: center">39.34</td>
      <td style="text-align: center">37.78</td>
      <td style="text-align: center">55.76</td>
      <td style="text-align: center">23.35</td>
      <td style="text-align: center">57.86</td>
      <td style="text-align: center">43.21</td>
      <td style="text-align: center">17.50</td>
      <td style="text-align: center">55.10</td>
      <td style="text-align: center">29.74</td>
      <td style="text-align: center">47.56</td>
      <td style="text-align: center">41.28</td>
    </tr>
  </tbody>
</table>

<p>As can be seen, Model2Vec significantly outperforms GloVe and BPEmb on all tasks, and even outperforms MiniLM, which is a much slower model, on some tasks.</p>

<p>In addition, we evaluated Model2Vec on a number of classification datasets that are not in MTEB. We also use these to benchmark the speed of the model. The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: center">Average</th>
      <th style="text-align: center">SST2</th>
      <th style="text-align: center">IMDB</th>
      <th style="text-align: center">TREC</th>
      <th style="text-align: center">AG News</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">bge-base-en-v1.5</td>
      <td style="text-align: center">90.00</td>
      <td style="text-align: center">91.54</td>
      <td style="text-align: center">91.88</td>
      <td style="text-align: center">85.16</td>
      <td style="text-align: center">91.45</td>
    </tr>
    <tr>
      <td style="text-align: left">all-MiniLM-L6-v2</td>
      <td style="text-align: center">84.10</td>
      <td style="text-align: center">83.95</td>
      <td style="text-align: center">81.36</td>
      <td style="text-align: center">81.31</td>
      <td style="text-align: center">89.77</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: center">82.23</td>
      <td style="text-align: center">80.92</td>
      <td style="text-align: center">84.56</td>
      <td style="text-align: center">75.27</td>
      <td style="text-align: center">88.17</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove_subword</td>
      <td style="text-align: center">81.95</td>
      <td style="text-align: center">82.84</td>
      <td style="text-align: center">85.96</td>
      <td style="text-align: center">70.51</td>
      <td style="text-align: center">88.49</td>
    </tr>
    <tr>
      <td style="text-align: left">BPEmb_50k_300d</td>
      <td style="text-align: center">81.15</td>
      <td style="text-align: center">80.42</td>
      <td style="text-align: center">84.04</td>
      <td style="text-align: center">71.25</td>
      <td style="text-align: center">88.92</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove</td>
      <td style="text-align: center">80.76</td>
      <td style="text-align: center">83.07</td>
      <td style="text-align: center">85.24</td>
      <td style="text-align: center">66.12</td>
      <td style="text-align: center">88.61</td>
    </tr>
    <tr>
      <td style="text-align: left">GloVe_300d</td>
      <td style="text-align: center">77.77</td>
      <td style="text-align: center">81.68</td>
      <td style="text-align: center">84.00</td>
      <td style="text-align: center">55.67</td>
      <td style="text-align: center">89.71</td>
    </tr>
  </tbody>
</table>

<p>Again, Model2Vec outperforms GloVe BPEmb on all tasks, and even shows similar performance to MiniLM.</p>

<p>The figure below shows the relationship between the number of sentences per second and the average classification score. The circle sizes correspond to the number of parameters in the models (larger = more parameters). This plot shows that the Model2Vec models are much faster than the other models, while still being competitive in terms of classification performance with the all-MiniLM-L6-v2 model.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/post_hf/speed_vs_accuracy.png" alt="SpeedvsAccuracy" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>The average accuracy over all classification datasets plotted against sentence per second. The circle size indicates model size.</em></td>
    </tr>
  </tbody>
</table>

<h4 id="ablations">Ablations</h4>

<p>To better understand the factors contributing to the performance of Model2Vec, we conducted a comprehensive set of ablation studies, covering various aspects of the model’s architecture and preprocessing methods. In these studies, we examined the impact of key elements such as PCA, Zipf weighting, and the use of Sentence Transformers versus regular transformer models. We also compared the performance of input embeddings versus output embeddings, since it would seem plausible that these should also work well. The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: right">Avg (All)</th>
      <th style="text-align: right">Avg (MTEB)</th>
      <th style="text-align: right">Class</th>
      <th style="text-align: right">Clust</th>
      <th style="text-align: right">PairClass</th>
      <th style="text-align: right">Rank</th>
      <th style="text-align: right">Ret</th>
      <th style="text-align: right">STS</th>
      <th style="text-align: right">Sum</th>
      <th style="text-align: right">Pearl</th>
      <th style="text-align: right">WordSim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: right">46.79</td>
      <td style="text-align: right">45.34</td>
      <td style="text-align: right">61.25</td>
      <td style="text-align: right">25.58</td>
      <td style="text-align: right">74.9</td>
      <td style="text-align: right">47.63</td>
      <td style="text-align: right">26.14</td>
      <td style="text-align: right">68.58</td>
      <td style="text-align: right">29.2</td>
      <td style="text-align: right">54.02</td>
      <td style="text-align: right">49.18</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output_nopca</td>
      <td style="text-align: right">44.04</td>
      <td style="text-align: right">42.31</td>
      <td style="text-align: right">61.42</td>
      <td style="text-align: right">20.15</td>
      <td style="text-align: right">68.21</td>
      <td style="text-align: right">44.67</td>
      <td style="text-align: right">25.25</td>
      <td style="text-align: right">61.87</td>
      <td style="text-align: right">29.85</td>
      <td style="text-align: right">51.02</td>
      <td style="text-align: right">48.96</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output_nozipf</td>
      <td style="text-align: right">43.61</td>
      <td style="text-align: right">41.52</td>
      <td style="text-align: right">60.44</td>
      <td style="text-align: right">21.62</td>
      <td style="text-align: right">72.15</td>
      <td style="text-align: right">45.57</td>
      <td style="text-align: right">20.35</td>
      <td style="text-align: right">62.71</td>
      <td style="text-align: right">30.66</td>
      <td style="text-align: right">52.28</td>
      <td style="text-align: right">49.17</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_input_nozipf_nopca</td>
      <td style="text-align: right">40.97</td>
      <td style="text-align: right">39.55</td>
      <td style="text-align: right">54.16</td>
      <td style="text-align: right">18.62</td>
      <td style="text-align: right">68.3</td>
      <td style="text-align: right">43.65</td>
      <td style="text-align: right">23.63</td>
      <td style="text-align: right">59.38</td>
      <td style="text-align: right">32.04</td>
      <td style="text-align: right">50.19</td>
      <td style="text-align: right">40.52</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output_nozipf_nopca</td>
      <td style="text-align: right">40.8</td>
      <td style="text-align: right">38.44</td>
      <td style="text-align: right">59.78</td>
      <td style="text-align: right">19.31</td>
      <td style="text-align: right">62.39</td>
      <td style="text-align: right">42.26</td>
      <td style="text-align: right">19.01</td>
      <td style="text-align: right">55.16</td>
      <td style="text-align: right">30</td>
      <td style="text-align: right">49.09</td>
      <td style="text-align: right">48.97</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_input</td>
      <td style="text-align: right">40.74</td>
      <td style="text-align: right">39.93</td>
      <td style="text-align: right">60.35</td>
      <td style="text-align: right">22.66</td>
      <td style="text-align: right">59.63</td>
      <td style="text-align: right">43.02</td>
      <td style="text-align: right">25.47</td>
      <td style="text-align: right">50.05</td>
      <td style="text-align: right">29.35</td>
      <td style="text-align: right">50.61</td>
      <td style="text-align: right">34.47</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_bert_output_nozipf_nopca</td>
      <td style="text-align: right">35.54</td>
      <td style="text-align: right">34.82</td>
      <td style="text-align: right">55.69</td>
      <td style="text-align: right">15.42</td>
      <td style="text-align: right">58.68</td>
      <td style="text-align: right">39.87</td>
      <td style="text-align: right">12.92</td>
      <td style="text-align: right">55.24</td>
      <td style="text-align: right">30.15</td>
      <td style="text-align: right">46.9</td>
      <td style="text-align: right">26.72</td>
    </tr>
  </tbody>
</table>

<p>There’s four main findings in these results:</p>
<ol>
  <li>Non-Sentence Transformers do not work well. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_bert_output_nozipf_nopca</code> (which uses <a href="https://huggingface.co/google-bert/bert-base-uncased">BERT</a>, a non-Sentence Transformer) and <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf_nopca</code> (which uses <a href="https://huggingface.co/BAAI/bge-base-en-v1.5">BGE-base</a>, a Sentence Transformer). Using a Sentence Transformer gives a ~5.2% increase in performance.</li>
  <li>PCA is crucial for performance. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf_nopca</code> and <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf</code> which gives a ~2.8% increase in performance. Furthermore, PCA improves performance on <em>all</em> tasks.</li>
  <li>Zipf weighting is crucial for performance. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_base_output_nozipf_nopca</code> and <code class="language-plaintext highlighter-rouge">M2V_base_output_nopca</code> which gives a ~3.1% increase in performance.</li>
  <li>Output embeddings outperform input embeddings. This can be seen by comparing <code class="language-plaintext highlighter-rouge">M2V_base_input</code> and <code class="language-plaintext highlighter-rouge">M2V_base_output</code> which gives a ~6.1% increase in performance. Note that input embeddings do work well for some tasks. We hypothesize that this is because input embeddings are inherently normalized.</li>
</ol>

<h3 id="conclusion">Conclusion</h3>

<p>Thanks for reading our blog post on Model2Vec! We hope you found it informative and useful. If you have any questions or comments, please feel free to reach out to us. We are still actively working on the project, and have a number of features already planned, so stay tuned.</p>

<ul>
  <li>💻 <a href="https://github.com/MinishLab/model2vec">Repository</a></li>
  <li>🤗 <a href="https://huggingface.co/minishlab">HuggingFace Org</a></li>
  <li>🤗 <a href="https://huggingface.co/collections/minishlab/model2vec-base-models-66fd9dd9b7c3b3c0f25ca90e">HuggingFace Models</a></li>
  <li>👥 <a href="https://www.linkedin.com/company/minish-lab">LinkedIn</a></li>
  <li>📚 <a href="https://github.com/MinishLab/model2vec/tree/main/tutorials">Tutorials</a></li>
</ul>

<h3 id="citing">Citing</h3>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024word2vec</span><span class="p">,</span>
  <span class="na">authors</span> <span class="p">=</span> <span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://github.com/MinishLab/model2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We’d like to thank <a href="https://huggingface.co/tomaarsen">Tom Aarsen</a> for integrating Model2Vec into <a href="https://github.com/UKPLab/sentence-transformers">Sentence Transformers</a> and helping us with our <a href="https://huggingface.co/minishlab">HuggingFace</a> integration, as well as his general feedback on the project.</p>

  </div>

  <!--<div class="date">
    Written on October 14, 2024
  </div>-->

</article>

        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            



<a href="https://github.com/minishlab" target="_blank"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/company/minish-lab" target="_blank"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml" target="_blank"><i class="svg-icon rss"></i></a>







          </footer>
        </div>
      </div>
    </div>

  </body>
</html>
