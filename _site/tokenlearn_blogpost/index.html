<!DOCTYPE html>
<html>
  <head>
    <title>Tokenlearn blogpost – Minish Lab – Small models everywhere</title>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Tokenlearn blogpost" />
<meta name="author" content="The Minish Lab" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blogpost describes the Tokenlearn method, which is a method to pre-train Model2Vec models." />
<meta property="og:description" content="This blogpost describes the Tokenlearn method, which is a method to pre-train Model2Vec models." />
<link rel="canonical" href="http://localhost:4000/tokenlearn_blogpost/" />
<meta property="og:url" content="http://localhost:4000/tokenlearn_blogpost/" />
<meta property="og:site_name" content="Minish Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-10-29T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tokenlearn blogpost" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"The Minish Lab"},"dateModified":"2024-10-29T00:00:00+01:00","datePublished":"2024-10-29T00:00:00+01:00","description":"This blogpost describes the Tokenlearn method, which is a method to pre-train Model2Vec models.","headline":"Tokenlearn blogpost","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/tokenlearn_blogpost/"},"url":"http://localhost:4000/tokenlearn_blogpost/"}</script>
<!-- End Jekyll SEO tag -->

    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>


<meta property="og:description" content="This blogpost describes the Tokenlearn method, which is a method to pre-train Model2Vec models.

" />

<meta name="author" content="Minish Lab" />


<meta property="og:title" content="Tokenlearn blogpost" />
<meta property="twitter:title" content="Tokenlearn blogpost" />



<meta property="og:image" content="http://localhost:4000/images/ezlo.png"/>
<meta property="twitter:image" content="http://localhost:4000/images/ezlo.png"/>



    <link rel="stylesheet" type="text/css" href="/assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Minish Lab - Small models everywhere" href="/feed.xml" />
    <link rel="canonical" href="http://localhost:4000/tokenlearn_blogpost/" />

    <meta name="theme-color" content="#000000">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="/" class="site-avatar"><img src="/images/ezlo.png" alt="" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="/">Minish Lab</a></h1>
              <p class="site-description">Small models everywhere</p> 
            </div>

            <nav>
              <a href="/">Home</a>
              <a href="/about">About</a>
              <a href="/archive">Archive</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <article class="post detailed">
  <h1>Tokenlearn blogpost</h1>

  <div>
    <p class="author_title">The Minish Lab  ·  October 29, 2024</p>
    
    <div class="post-tags">
      
    </div>
  </div>
    
  <div class="entry">
    <p>This blogpost describes the <a href="https://github.com/MinishLab/tokenlearn">Tokenlearn</a> method, which is a method to pre-train Model2Vec models.</p>

<h1 id="bag-of-tricks-leads-to-better-models-potion">Bag of tricks leads to better models: POTION</h1>

<p>We’ve been brewing, concocting, distilling, and came up with a new distillation technique that leads to much better models, which we are now releasing under the name POTION. We open source all models, code, and data.</p>

<p>We’re releasing three versions: a 64-dim (1.9M params), 128-dim (3.8M params), and 256-dim (7.6M params) model, all based on the same base model, which is, in turn, a bge-base distillation. All POTION models outperform all previous distillations in their size class, and should be considered to be drop-in replacements of our M2V_base_output model. potion-base-8M, in particular, even improves over our largest model, M2V_base_glove. potion-base-8M is better than any set of static embeddings we could find on any task, including glove, fasttext and specialized word embeddings.</p>

<p>Get them here:</p>
<ul>
  <li><a href="https://huggingface.co/MinishLab/potion-base-8M">potion-base-8M</a></li>
  <li><a href="https://huggingface.co/MinishLab/potion-base-4M">potion-base-4M</a></li>
  <li><a href="https://huggingface.co/MinishLab/potion-base-2M">potion-base-2M</a></li>
</ul>

<p>The Tokenlearn code can be found <a href="https://github.com/MinishLab/tokenlearn">here</a>.</p>

<p>The rest of the post will detail how we made the models, how they perform, and further improvements we have in store.</p>

<h2 id="distillation">Distillation</h2>

<p>In our regular <a href="https://github.com/MinishLab/model2vec">model2vec</a> framework we distill sentence transformers down to really fast tiny models by doing a forward pass for all tokens separately. We then perform Principal Component Analysis (PCA) on the resulting embeddings, and weigh the individual embeddings via Zipf’s law. See our previous blog post <a href="https://huggingface.co/blog/Pringled/model2vec">here</a>. The new distillation framework is composed of 4 steps.</p>

<ol>
  <li>Model2Vec distillation</li>
  <li>Sentence transformer inference</li>
  <li>Training</li>
  <li>Post-training regularization</li>
</ol>

<p>These four steps take a bit longer than the previous distillation framework. If you are looking for a quick way to get a model2vec model, distillation is still your best bet. If you are looking for maximum performance, read on!</p>

<h3 id="1-distillation">1. Distillation</h3>

<p>We start from a distilled model. In our case, we are using the M2V_base_output model as our starting point.</p>

<h3 id="2-sentence-transformer-inference">2. Sentence transformer inference</h3>

<p>We then go back to the original big sentence transformer, and use that transformer to create ~1M embeddings on an in-domain corpus, which for us is <a href="https://huggingface.co/datasets/allenai/c4">C4</a>. We then throw away the sentence transformer, never to see it again. Forget it existed.</p>

<h3 id="3-training">3. Training</h3>

<p>So, we now have a base model, and 1M texts and 1M vector representations of those texts. We then train the base model to minimize the cosine distance between the representations it produces and the representations we produced before. In doing so, our model learns to better mimic representations made by a large model. We also add a super heavy regularization term to the produced embeddings.</p>

<p>During training, we apply a few standard methods to improve performance, such as reducing the learning rate on plateau, and early stopping.</p>

<h3 id="4-post-training-re-regularization">4. Post-training re-regularization</h3>

<p>Finally, after training, we <em>re-regularize</em> our models by performing PCA, and by manually re-weighting individual tokens. As we show below, this massively improves performance, again.</p>

<p>Of note here is the manual re-weighting, which is very similar to the Zipf weighting we use, but now relies on external data. Before, we assumed that all tokens were in rank order, and simply weighted them as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">rank</span><span class="p">)</span>
</code></pre></div></div>

<p>This works really well, as shown in <a href="https://huggingface.co/blog/Pringled/model2vec">our original blog post</a>. Using actual frequencies, however, works even better. We use the same 1M documents on which we trained, and collect token probabilities for all tokens in our vocabulary. We then reweight using the following formula from the <a href="https://openreview.net/pdf?id=SyK00v5xx">SIF paper</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="mf">1e-3</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1e-3</span> <span class="o">+</span> <span class="n">proba</span><span class="p">)</span>
</code></pre></div></div>

<p>where <code class="language-plaintext highlighter-rouge">proba</code> is the probability of the token in the corpus. While this does mean our new distillation method relies on some data, it is <em>worth it</em>, as we will show below.</p>

<h2 id="results">Results</h2>

<p>Just like in our original experiments, we again evaluate on MTEB, as well as our two additional tasks (PEARL and WordSim). The results are shown in the table below.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: right">Avg (All)</th>
      <th style="text-align: right">Avg (MTEB)</th>
      <th style="text-align: right">Class</th>
      <th style="text-align: right">Clust</th>
      <th style="text-align: right">PairClass</th>
      <th style="text-align: right">Rank</th>
      <th style="text-align: right">Ret</th>
      <th style="text-align: right">STS</th>
      <th style="text-align: right">Sum</th>
      <th style="text-align: right">Pearl</th>
      <th style="text-align: right">WordSim</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">all-MiniLM-L6-v2</td>
      <td style="text-align: right">56.08</td>
      <td style="text-align: right">56.09</td>
      <td style="text-align: right">62.62</td>
      <td style="text-align: right">41.94</td>
      <td style="text-align: right">82.37</td>
      <td style="text-align: right">58.04</td>
      <td style="text-align: right">41.95</td>
      <td style="text-align: right">78.90</td>
      <td style="text-align: right">30.81</td>
      <td style="text-align: right">60.83</td>
      <td style="text-align: right">49.91</td>
    </tr>
    <tr>
      <td style="text-align: left">potion-base-8M</td>
      <td style="text-align: right">50.54</td>
      <td style="text-align: right">50.03</td>
      <td style="text-align: right">64.44</td>
      <td style="text-align: right">32.93</td>
      <td style="text-align: right">76.62</td>
      <td style="text-align: right">49.73</td>
      <td style="text-align: right">31.71</td>
      <td style="text-align: right">73.24</td>
      <td style="text-align: right">29.28</td>
      <td style="text-align: right">53.54</td>
      <td style="text-align: right">50.75</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove_subword</td>
      <td style="text-align: right">49.06</td>
      <td style="text-align: right">46.69</td>
      <td style="text-align: right">61.27</td>
      <td style="text-align: right">30.03</td>
      <td style="text-align: right">74.71</td>
      <td style="text-align: right">49.15</td>
      <td style="text-align: right">27.16</td>
      <td style="text-align: right">69.09</td>
      <td style="text-align: right">30.08</td>
      <td style="text-align: right">56.82</td>
      <td style="text-align: right">57.99</td>
    </tr>
    <tr>
      <td style="text-align: left">potion-base-4M</td>
      <td style="text-align: right">48.87</td>
      <td style="text-align: right">48.23</td>
      <td style="text-align: right">62.19</td>
      <td style="text-align: right">31.47</td>
      <td style="text-align: right">75.37</td>
      <td style="text-align: right">48.75</td>
      <td style="text-align: right">29.11</td>
      <td style="text-align: right">72.19</td>
      <td style="text-align: right">28.89</td>
      <td style="text-align: right">52.55</td>
      <td style="text-align: right">49.21</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_glove</td>
      <td style="text-align: right">48.58</td>
      <td style="text-align: right">47.6</td>
      <td style="text-align: right">61.35</td>
      <td style="text-align: right">30.52</td>
      <td style="text-align: right">75.34</td>
      <td style="text-align: right">48.5</td>
      <td style="text-align: right">29.26</td>
      <td style="text-align: right">70.31</td>
      <td style="text-align: right">31.5</td>
      <td style="text-align: right">50.28</td>
      <td style="text-align: right">54.29</td>
    </tr>
    <tr>
      <td style="text-align: left">M2V_base_output</td>
      <td style="text-align: right">46.79</td>
      <td style="text-align: right">45.34</td>
      <td style="text-align: right">61.25</td>
      <td style="text-align: right">25.58</td>
      <td style="text-align: right">74.9</td>
      <td style="text-align: right">47.63</td>
      <td style="text-align: right">26.14</td>
      <td style="text-align: right">68.58</td>
      <td style="text-align: right">29.2</td>
      <td style="text-align: right">54.02</td>
      <td style="text-align: right">49.18</td>
    </tr>
    <tr>
      <td style="text-align: left">potion-base-2M</td>
      <td style="text-align: right">45.52</td>
      <td style="text-align: right">44.77</td>
      <td style="text-align: right">58.45</td>
      <td style="text-align: right">27.5</td>
      <td style="text-align: right">73.72</td>
      <td style="text-align: right">46.82</td>
      <td style="text-align: right">24.13</td>
      <td style="text-align: right">70.14</td>
      <td style="text-align: right">31.51</td>
      <td style="text-align: right">50.82</td>
      <td style="text-align: right">44.72</td>
    </tr>
    <tr>
      <td style="text-align: left">GloVe_300d</td>
      <td style="text-align: right">42.84</td>
      <td style="text-align: right">42.36</td>
      <td style="text-align: right">57.31</td>
      <td style="text-align: right">27.66</td>
      <td style="text-align: right">72.48</td>
      <td style="text-align: right">43.3</td>
      <td style="text-align: right">22.78</td>
      <td style="text-align: right">61.9</td>
      <td style="text-align: right">28.81</td>
      <td style="text-align: right">45.65</td>
      <td style="text-align: right">43.05</td>
    </tr>
    <tr>
      <td style="text-align: left">BPEmb_50k_300d</td>
      <td style="text-align: right">39.34</td>
      <td style="text-align: right">37.78</td>
      <td style="text-align: right">55.76</td>
      <td style="text-align: right">23.35</td>
      <td style="text-align: right">57.86</td>
      <td style="text-align: right">43.21</td>
      <td style="text-align: right">17.5</td>
      <td style="text-align: right">55.1</td>
      <td style="text-align: right">29.74</td>
      <td style="text-align: right">47.56</td>
      <td style="text-align: right">41.28</td>
    </tr>
  </tbody>
</table>

<p>As can be seen, potion-base-8M is the best model we have released so far (surpassing the 50% average MTEB score mark!), further pushing the limits of what is possible with static word embeddings. Furthermore, the 4M and 2M models still work quite well, with the 2M model outperforming GloVE while being ~55 times smaller.</p>

<p>To show the relationship between the number of sentences per second and the average MTEB score, we plot the average MTEB score against sentences per second. The circle sizes correspond to the number of parameters in the models (larger = more parameters).</p>

<p><img src="/images/post_tokenlearn/speed_vs_mteb_score.png" alt="SpeedvsAccuracy" /> 
<em>The average MTEB score plotted against sentences per second. The circle size indicates model size.</em></p>

  </div>

  <!--<div class="date">
    Written on October 29, 2024
  </div>-->

</article>

        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            



<a href="https://github.com/minishlab" target="_blank"><i class="svg-icon github"></i></a>



<a href="/feed.xml" target="_blank"><i class="svg-icon rss"></i></a>







          </footer>
        </div>
      </div>
    </div>

  </body>
</html>
